"""
OCR æ–‡å­—æ¯”å°ç¨‹å¼ - å¤šèªè¨€æ”¯æ´ç‰ˆæœ¬
åŠŸèƒ½: åœ–ç‰‡é€ä¸€è™•ç† -> å¤šèªè¨€OCR -> èªè¨€åŒ¹é… -> è¦–è¦ºæ¨™è¨˜ -> çµæœå ±å‘Š
"""

import os, csv, time, json, sys
from PIL import Image, ImageDraw
from text_extraction import main as text_extraction_main

# å…¨åŸŸåƒæ•¸
INPUT_TARGET_DIR = r"input data\target"
INPUT_TEXT_FILE = r"input data\text\Checklist_listed_text.txt"
RESULT_DIR = r"result"
OCR_CONFIDENCE_THRESHOLD = 0.3
IGNORED_CHARS = ['=']

def load_images():
    """è¼‰å…¥åœ–ç‰‡æª”æ¡ˆ"""
    print(f"è¼‰å…¥åœ–ç‰‡: {INPUT_TARGET_DIR}")
    if not os.path.exists(INPUT_TARGET_DIR):
        print(f"  [ERROR] ç›®éŒ„ä¸å­˜åœ¨: {INPUT_TARGET_DIR}")
        return []
    
    formats = {'.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.gif'}
    images = [os.path.join(INPUT_TARGET_DIR, f) for f in os.listdir(INPUT_TARGET_DIR) 
             if os.path.splitext(f.lower())[1] in formats]
    print(f"  [OK] æ‰¾åˆ° {len(images)} å¼µåœ–ç‰‡")
    return images

def parse_target_texts():
    """è§£æç›®æ¨™æ–‡å­—ï¼Œå›å‚³ç›®æ¨™åˆ—è¡¨å’Œèªè¨€é›†åˆ"""
    print(f"è§£æç›®æ¨™æ–‡å­—: {INPUT_TEXT_FILE}")
    if not os.path.exists(INPUT_TEXT_FILE):
        print(f"  [ERROR] æª”æ¡ˆä¸å­˜åœ¨: {INPUT_TEXT_FILE}")
        return [], set()
    
    targets = []
    languages = set()
    
    with open(INPUT_TEXT_FILE, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            
            if ',' in line:
                text, language = line.rsplit(',', 1)
                text, language = text.strip(), language.strip()
            else:
                text, language = line, 'eng'
                print(f"  [WARNING] ç¬¬{line_num}è¡Œç¼ºå°‘èªè¨€æ¨™è¨˜ï¼Œé è¨­ç‚ºeng")
            
            targets.append({'text': text, 'language': language, 'line_number': line_num})
            languages.add(language)
    
    print(f"  [OK] ç›®æ¨™: {len(targets)}å€‹, èªè¨€: {sorted(languages)}")
    return targets, languages

def run_multi_language_ocr(image_path, languages):
    """å°å–®å¼µåœ–ç‰‡åŸ·è¡Œå¤šèªè¨€OCR"""
    image_name = os.path.basename(image_path)
    print(f"  å¤šèªè¨€OCRè™•ç†: {image_name}")
    
    # å°å–®å¼µåœ–ç‰‡åŸ·è¡ŒOCRè™•ç†
    for language in languages:
        print(f"    {language.upper()}...", end=" ")
        try:
            success = text_extraction_main(
                test_image=image_path,
                test_language=language,
                confidence_threshold=OCR_CONFIDENCE_THRESHOLD,
                result_path=RESULT_DIR
            )
            
            if success:
                print("âœ“")
            else:
                print("âœ—")
        except Exception as e:
            print(f"âœ— ({str(e)[:30]})")

def match_texts_comprehensive(image_name, targets):
    """ç¶œåˆåŒ¹é…åˆ†æï¼Œç”Ÿæˆè©³ç´°JSONè³‡æ–™"""
    print(f"  æ–‡å­—åŒ¹é…åˆ†æ...", end=" ")
    image_basename = os.path.splitext(image_name)[0]
    
    # å…§éƒ¨è¼”åŠ©å‡½æ•¸
    def calculate_bbox_from_words(words):
        """å¾å–®è©åˆ—è¡¨è¨ˆç®—çµ±ä¸€é‚Šç•Œæ¡†"""
        if not words:
            return None
        
        min_x = min(w['bbox']['x'] for w in words)
        max_x = max(w['bbox']['x'] + w['bbox']['width'] for w in words)
        min_y = min(w['bbox']['y'] for w in words)
        max_y = max(w['bbox']['y'] + w['bbox']['height'] for w in words)
        
        return {
            'x': min_x,
            'y': min_y,
            'width': max_x - min_x,
            'height': max_y - min_y
        }
    
    def normalize_text(text):
        """æ–‡å­—æ­£è¦åŒ–ï¼šç§»é™¤ç‰¹æ®Šå­—ç¬¦ä¸¦æ¨™æº–åŒ–ç©ºæ ¼"""
        normalized = text
        for char in IGNORED_CHARS:
            normalized = normalized.replace(char, ' ')
        return ' '.join(normalized.split())
    
    def find_target_in_words(target_text, words):
        """åœ¨å–®è©åˆ—è¡¨ä¸­å°‹æ‰¾ç›®æ¨™æ–‡å­—çš„ç²¾ç¢ºé‚Šç•Œæ¡†"""
        target_normalized = normalize_text(target_text)
        target_words = target_normalized.split()
        
        if len(target_words) == 0:
            return None
        
        # å»ºç«‹å®Œæ•´çš„å–®è©æ–‡å­—ä¸²åˆ—
        word_texts = [normalize_text(w.get('text', '')) for w in words]
        
        # å°‹æ‰¾ç›®æ¨™æ–‡å­—çš„èµ·å§‹ä½ç½®
        for i in range(len(word_texts)):
            # æª¢æŸ¥å¾ç¬¬iå€‹å–®è©é–‹å§‹æ˜¯å¦åŒ¹é…ç›®æ¨™æ–‡å­—
            if i + len(target_words) <= len(word_texts):
                match_words = word_texts[i:i+len(target_words)]
                if match_words == target_words:
                    # æ‰¾åˆ°åŒ¹é…ï¼Œä½¿ç”¨çµ±ä¸€å‡½æ•¸è¨ˆç®—é‚Šç•Œæ¡†
                    matched_words = words[i:i+len(target_words)]
                    return calculate_bbox_from_words(matched_words)
        
        return None
    
    def calculate_target_bbox(target_text, full_text, full_bbox, words=None):
        """è¨ˆç®—ç›®æ¨™æ–‡å­—åœ¨å®Œæ•´æ–‡å­—ä¸­çš„é‚Šç•Œæ¡†ä½ç½®"""
        # å¦‚æœæœ‰å–®è©ç´šé‚Šç•Œæ¡†ï¼Œå„ªå…ˆä½¿ç”¨ç²¾ç¢ºåŒ¹é…
        if words:
            target_bbox = find_target_in_words(target_text, words)
            if target_bbox:
                return target_bbox
        
        # æ‰¾åˆ°ç›®æ¨™æ–‡å­—åœ¨å®Œæ•´æ–‡å­—ä¸­çš„èµ·å§‹ä½ç½®
        start_index = full_text.find(target_text)
        if start_index == -1:
            return full_bbox  # å¦‚æœæ‰¾ä¸åˆ°ï¼Œè¿”å›å®Œæ•´é‚Šç•Œæ¡†
        
        # è¨ˆç®—ç›®æ¨™æ–‡å­—çš„é•·åº¦æ¯”ä¾‹
        target_length = len(target_text)
        full_length = len(full_text)
        
        if full_length == 0:
            return full_bbox
        
        # è¨ˆç®—ç›¸å°ä½ç½®å’Œå¯¬åº¦
        start_ratio = start_index / full_length
        length_ratio = target_length / full_length
        
        # è¨ˆç®—æ–°çš„é‚Šç•Œæ¡†
        new_x = full_bbox['x'] + int(full_bbox['width'] * start_ratio)
        new_width = max(int(full_bbox['width'] * length_ratio), 10)  # æœ€å°å¯¬åº¦10åƒç´ 
        
        return {
            'x': new_x,
            'y': full_bbox['y'],
            'width': new_width,
            'height': full_bbox['height']
        }
    
    def calculate_combo_word_positions(target_normalized, combo_texts):
        """è¨ˆç®—çµ„åˆåŒ¹é…ä¸­ç›®æ¨™æ–‡å­—åœ¨å„è¡Œçš„ä½ç½®"""
        word_positions = []
        target_words = target_normalized.split()
        
        # å»ºç«‹æ‰€æœ‰è¡Œçš„è©èªåˆ—è¡¨
        all_words = []
        
        for line_idx, line_data in enumerate(combo_texts):
            line_words = line_data.get('words', [])
            for word in line_words:
                all_words.append({
                    'text': normalize_text(word.get('text', '')),
                    'bbox': word.get('bbox', {}),
                    'line_idx': line_idx
                })
        
        # åœ¨æ‰€æœ‰è©èªä¸­å°‹æ‰¾ç›®æ¨™æ–‡å­—åºåˆ—
        for i in range(len(all_words) - len(target_words) + 1):
            match_words = [w['text'] for w in all_words[i:i+len(target_words)]]
            if match_words == target_words:
                # æ‰¾åˆ°åŒ¹é…ï¼ŒæŒ‰è¡Œçµ„ç¹”ä½ç½®
                matched_words = all_words[i:i+len(target_words)]
                
                # æŒ‰è¡Œåˆ†çµ„
                line_groups = {}
                for word in matched_words:
                    line_idx = word['line_idx']
                    if line_idx not in line_groups:
                        line_groups[line_idx] = []
                    line_groups[line_idx].append(word)
                
                # ç‚ºæ¯è¡Œè¨ˆç®—é‚Šç•Œæ¡†
                for line_idx, words_in_line in line_groups.items():
                    if words_in_line:
                        line_text = ' '.join([w['text'] for w in words_in_line])
                        line_bbox = calculate_bbox_from_words(words_in_line)
                        
                        word_positions.append({
                            'text': line_text,
                            'bbox': line_bbox
                        })
                break
        
        return word_positions
    
    def add_visual_markup(target_text, bbox, match_type, segment_text=None):
        """çµ±ä¸€çš„è¦–è¦ºæ¨™è¨˜æ·»åŠ å‡½æ•¸"""
        markup_data = {
            'target_text': target_text,
            'bbox': bbox,
            'match_type': match_type
        }
        if segment_text:
            markup_data['segment_text'] = segment_text
        comprehensive_data['visual_markup_data']['markup_positions'].append(markup_data)
    
    def process_match(target_text, extracted_text, confidence, match_type, processing_note, target_bbox, word_positions=None):
        """çµ±ä¸€è™•ç†åŒ¹é…çµæœ"""
        match_result.update({
            'status': 'æˆåŠŸ',
            'extracted_text': extracted_text,
            'confidence': confidence,
            'match_type': match_type,
            'processing_note': processing_note
        })
        
        # æ·»åŠ è¦–è¦ºæ¨™è¨˜
        if match_type == 'çµ„åˆåŒ¹é…' and word_positions:
            for word_pos in word_positions:
                add_visual_markup(target_text, word_pos['bbox'], match_type, word_pos['text'])
        else:
            add_visual_markup(target_text, target_bbox, match_type)
        
        return True
    
    # è©³ç´°åŒ¹é…è³‡æ–™çµæ§‹
    comprehensive_data = {
        'image_info': {
            'filename': image_name,
            'basename': image_basename,
            'processing_time': time.strftime('%Y-%m-%d %H:%M:%S'),
            'total_targets': len(targets)
        },
        'matching_results': {
            'matches': [],
            'summary_stats': {'total_targets': len(targets), 'matched_count': 0, 'failed_count': 0}
        },
        'visual_markup_data': {
            'markup_positions': []
        }
    }
    
    for target in targets:
        target_text, target_language = target['text'], target['language']
        json_file = os.path.join(RESULT_DIR, f"{image_basename}_ocr_text_{target_language}.json")
        
        match_result = {
            'target_text': target_text,
            'language': target_language,
            'ocr_source_file': os.path.basename(json_file) if os.path.exists(json_file) else 'N/A',
            'status': 'å¤±æ•—',
            'extracted_text': 'N/A',
            'confidence': 'N/A',
            'match_type': 'N/A',
            'processing_note': 'ç„¡è™•ç†'
        }
        
        if os.path.exists(json_file):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    ocr_data = json.load(f)
                
                target_normalized = normalize_text(target_text)
                
                for img_info in ocr_data['images']:
                    matched = False
                    
                    # å–®è¡ŒåŒ¹é…
                    for extracted in img_info['texts']:
                        text, confidence, bbox = extracted['text'], extracted['confidence'], extracted['bbox']
                        words = extracted.get('words', [])
                        text_normalized = normalize_text(text)
                        
                        processing_note = 'å¿½ç•¥ç‰¹æ®Šå­—ç¬¦/ç©ºæ ¼æ¨™æº–åŒ–' if (target_text != target_normalized or text != text_normalized) else 'ç„¡è™•ç†'
                        
                        if target_normalized == text_normalized:
                            match_type = 'å®Œå…¨åŒ¹é…'
                            target_bbox = bbox
                        elif target_normalized in text_normalized:
                            match_type = 'åŒ…å«åŒ¹é…'
                            target_bbox = calculate_target_bbox(target_normalized, text_normalized, bbox, words)
                        else:
                            continue
                        
                        matched = process_match(target_text, text, confidence, match_type, processing_note, target_bbox)
                        break
                    
                    # çµ„åˆåŒ¹é… (å¦‚æœå–®è¡ŒåŒ¹é…å¤±æ•—)
                    if not matched:
                        for combo_size in [2, 3]:  # 2è¡Œçµ„åˆ, 3è¡Œçµ„åˆ
                            if matched:  # å¦‚æœå·²ç¶“åŒ¹é…ï¼Œè·³å‡ºå¤–å±¤å¾ªç’°
                                break
                            for i in range(len(img_info['texts']) - combo_size + 1):
                                combo_texts = img_info['texts'][i:i+combo_size]
                                combined_text = ' '.join([t['text'] for t in combo_texts])
                                combined_normalized = normalize_text(combined_text)
                                
                                if target_normalized in combined_normalized:
                                    word_positions = calculate_combo_word_positions(target_normalized, combo_texts)
                                    if word_positions:
                                        first_bbox = combo_texts[0]['bbox']
                                        avg_confidence = sum(t['confidence'] for t in combo_texts) / len(combo_texts)
                                        matched = process_match(target_text, combined_text, avg_confidence, 'çµ„åˆåŒ¹é…', 'ç„¡è™•ç†', first_bbox, word_positions)
                                        break
                    
                    if matched:
                        break
                        
            except Exception as e:
                print(f"      [ERROR] è®€å–{os.path.basename(json_file)}å¤±æ•—: {e}")
        
        comprehensive_data['matching_results']['matches'].append(match_result)
        
        # æ›´æ–°çµ±è¨ˆ
        stats = comprehensive_data['matching_results']['summary_stats']
        if match_result['status'] == 'æˆåŠŸ':
            stats['matched_count'] += 1
        else:
            stats['failed_count'] += 1
    
    # ä¿å­˜è©³ç´°åŒ¹é…è³‡æ–™ç‚ºJSON
    comprehensive_json_file = os.path.join(RESULT_DIR, f"{image_basename}_comprehensive_match_data.json")
    with open(comprehensive_json_file, 'w', encoding='utf-8') as f:
        json.dump(comprehensive_data, f, ensure_ascii=False, indent=2)
    
    success_count = comprehensive_data['matching_results']['summary_stats']['matched_count']
    print(f"{success_count}/{len(targets)}")
    return comprehensive_data, comprehensive_json_file

def generate_csv_from_comprehensive_data(comprehensive_json_file):
    """å¾è©³ç´°åŒ¹é…JSONç”ŸæˆCSVå ±å‘Š"""
    print(f"  ç”ŸæˆCSVå ±å‘Š...", end=" ")
    
    try:
        with open(comprehensive_json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        image_basename = data['image_info']['basename']
        csv_file = os.path.join(RESULT_DIR, f"{image_basename}_text_detection_report.csv")
        
        fieldnames = ['ç›®æ¨™æ–‡å­—', 'èªè¨€', 'åŒ¹é…JSONæª”', 'æª¢æ¸¬ç‹€æ…‹', 
                      'è­˜åˆ¥æ–‡å­—', 'ä¿¡å¿ƒåº¦', 'åŒ¹é…é¡å‹', 'è™•ç†æ¨™è¨˜']
        
        with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            
            for match in data['matching_results']['matches']:
                writer.writerow({
                    'ç›®æ¨™æ–‡å­—': match['target_text'],
                    'èªè¨€': match['language'],
                    'åŒ¹é…JSONæª”': match['ocr_source_file'],
                    'æª¢æ¸¬ç‹€æ…‹': match['status'],
                    'è­˜åˆ¥æ–‡å­—': match['extracted_text'],
                    'ä¿¡å¿ƒåº¦': match['confidence'],
                    'åŒ¹é…é¡å‹': match['match_type'],
                    'è™•ç†æ¨™è¨˜': match['processing_note']
                })
        
        match_count = len(data['matching_results']['matches'])
        print(f"âœ“ ({match_count}ç­†)")
        return csv_file
        
    except Exception as e:
        print(f"âœ— éŒ¯èª¤: {e}")
        return None

def create_visual_markup_from_comprehensive_data(image_path, comprehensive_json_file):
    """å¾è©³ç´°åŒ¹é…JSONåœ¨åœ–ç‰‡ä¸Šæ¨™è¨˜åŒ¹é…çµæœ"""
    image_name = os.path.basename(image_path)
    print(f"  è¦–è¦ºæ¨™è¨˜...", end=" ")
    
    try:
        # è®€å–è©³ç´°åŒ¹é…è³‡æ–™
        with open(comprehensive_json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        img = Image.open(image_path)
        draw = ImageDraw.Draw(img)
        
        # ä½¿ç”¨è¦–è¦ºæ¨™è¨˜è³‡æ–™é€²è¡Œç¹ªåœ–
        markup_positions = data['visual_markup_data']['markup_positions']
        marked_targets = set()
        
        for markup in markup_positions:
            bbox = markup['bbox']
            x1, x2 = bbox['x'], bbox['x'] + bbox['width']
            y = bbox['y'] + bbox['height'] + 2
            draw.line([x1, y, x2, y], fill=(255, 0, 0), width=2)
            marked_targets.add(markup['target_text'])
        
        image_basename = os.path.splitext(image_name)[0]
        marked_path = os.path.join(RESULT_DIR, f"{image_basename}_marked.png")
        img.save(marked_path)
        print(f"âœ“ ({len(marked_targets)}å€‹ç›®æ¨™)")
        
    except Exception as e:
        print(f"âœ— æ¨™è¨˜å¤±æ•—: {e}")

def main():
    """ä¸»ç¨‹å¼æµç¨‹"""
    print("ğŸ” OCR æ–‡å­—æ¯”å°ç¨‹å¼\n")
    start_time = time.time()
    
    # æ­¥é©Ÿ1: è¼‰å…¥åœ–ç‰‡
    image_files = load_images()
    if not image_files:
        return False
    
    # æ­¥é©Ÿ2: è§£æç›®æ¨™æ–‡å­—
    targets, languages = parse_target_texts()
    if not targets:
        return False
    
    all_results = []
    
    # æ­¥é©Ÿ3-7: é€å¼µåœ–ç‰‡è™•ç†
    for image_path in image_files:
        image_name = os.path.basename(image_path)
        print(f"\nğŸ“· {image_name}")
        
        # æ­¥é©Ÿ3: å¤šèªè¨€OCR
        run_multi_language_ocr(image_path, languages)
        
        # æ­¥é©Ÿ4: ç¶œåˆåŒ¹é…åˆ†æ
        comprehensive_data, json_file = match_texts_comprehensive(image_name, targets)
        
        # æ­¥é©Ÿ5: å¾JSONç”ŸæˆCSVå ±å‘Š
        generate_csv_from_comprehensive_data(json_file)
        
        # æ­¥é©Ÿ6: å¾JSONç”Ÿæˆè¦–è¦ºæ¨™è¨˜
        create_visual_markup_from_comprehensive_data(image_path, json_file)
        
        # ç´¯è¨ˆçµ±è¨ˆ
        all_results.extend(comprehensive_data['matching_results']['matches'])
    
    # çµæœçµ±è¨ˆ
    total_time = time.time() - start_time
    success_count = sum(1 for r in all_results if r['status'] == 'æˆåŠŸ')
    print(f"\nâœ… å®Œæˆ ({total_time:.1f}ç§’) - åŒ¹é…: {success_count}/{len(all_results)}")
    return True

if __name__ == "__main__":
    print("ğŸŒ å¤šèªè¨€ OCR ç³»çµ±\n")
    
    # æª¢æŸ¥æª”æ¡ˆ
    checks = [
        ('OCRæ¨¡çµ„', 'text_extraction.py'),
        ('åœ–ç‰‡ç›®éŒ„', INPUT_TARGET_DIR),
        ('ç›®æ¨™æ–‡å­—', INPUT_TEXT_FILE)
    ]
    
    for name, path in checks:
        if not os.path.exists(path):
            print(f"âŒ æ‰¾ä¸åˆ°{name}: {path}")
            sys.exit(1)
        print(f"âœ“ {name}")
    
    print()
    
    # åŸ·è¡Œä¸»ç¨‹å¼
    try:
        success = main()
        if success:
            print("\nç¨‹å¼åŸ·è¡ŒæˆåŠŸ")
        else:
            print("\nç¨‹å¼åŸ·è¡Œå¤±æ•—")
    except KeyboardInterrupt:
        print("\nç¨‹å¼è¢«ä¸­æ–·")
    except Exception as e:
        print(f"\n[ERROR] ç¨‹å¼éŒ¯èª¤: {e}")